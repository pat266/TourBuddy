{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "uZR3iGJJtdDE",
      "metadata": {
        "id": "uZR3iGJJtdDE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c875c80-fc4a-430a-b92f-ac35df702129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain==0.0.333 openai==1.2.2\n",
        "!pip -q install duckduckgo-search\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wPdWz1IdxyBR",
      "metadata": {
        "id": "wPdWz1IdxyBR"
      },
      "source": [
        "Setting up some keys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k_oRniCLjpN",
        "outputId": "41155c10-5fa2-433f-c28b-2664521b0e05"
      },
      "id": "0k_oRniCLjpN",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: langchain\n",
            "Version: 0.0.333\n",
            "Summary: Building applications with LLMs through composability\n",
            "Home-page: https://github.com/langchain-ai/langchain\n",
            "Author: \n",
            "Author-email: \n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: aiohttp, anyio, async-timeout, dataclasses-json, jsonpatch, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1571b632",
      "metadata": {
        "id": "1571b632"
      },
      "source": [
        "\n",
        "# Custom Tools & Agents ðŸ¤–"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c02c4fa2",
      "metadata": {
        "id": "c02c4fa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e078b1dc-e546-43a9-a0e7-fa11b5a8c29d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API key retrieved successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv('.env')\n",
        "\n",
        "# Retrieve the API key\n",
        "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "if openai_api_key is not None and len(openai_api_key) != 0:\n",
        "    print(\"API key retrieved successfully.\")\n",
        "else:\n",
        "    print(\"API key not found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "73bfcbb6",
      "metadata": {
        "id": "73bfcbb6"
      },
      "outputs": [],
      "source": [
        "# from langchain import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the turbo LLM\n",
        "turbo_llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-1106\") # 16k tokens sent, 4k tokens received"
      ],
      "metadata": {
        "id": "Lald4gCltB4V"
      },
      "id": "Lald4gCltB4V",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard Tool"
      ],
      "metadata": {
        "id": "7DKXFGHhxNcK"
      },
      "id": "7DKXFGHhxNcK"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools import DuckDuckGoSearchRun, BaseTool\n",
        "from langchain.agents import initialize_agent, AgentType, Tool, AgentExecutor\n",
        "from langchain.document_loaders import RecursiveUrlLoader\n",
        "from langchain.chains import LLMMathChain\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import re\n",
        "import requests\n",
        "\n",
        "search = DuckDuckGoSearchRun()\n",
        "llm_math_chain = LLMMathChain.from_llm(llm=turbo_llm, verbose=True)"
      ],
      "metadata": {
        "id": "kOVncXZtxT_0"
      },
      "id": "kOVncXZtxT_0",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class WebPageTool(BaseTool):\n",
        "#     name = \"Get Webpage\"\n",
        "#     description = \"Useful for when you need to get the content from a specific webpage\"\n",
        "\n",
        "#     def _run(self, webpage: str):\n",
        "#         response = requests.get(webpage)\n",
        "#         html_content = response.text\n",
        "\n",
        "#         def strip_html_tags(html_content):\n",
        "#             soup = bs(html_content, \"html.parser\")\n",
        "#             stripped_text = soup.get_text()\n",
        "#             return stripped_text\n",
        "\n",
        "#         stripped_content = strip_html_tags(html_content)\n",
        "#         if len(stripped_content) > 4000:\n",
        "#             stripped_content = stripped_content[:4000]\n",
        "#         return stripped_content\n",
        "\n",
        "#     def _arun(self, webpage: str):\n",
        "#         raise NotImplementedError(\"This tool does not support async\")\n",
        "\n",
        "# page_getter = WebPageTool()\n",
        "\n",
        "def extract_text_and_limit_tokens(html, token_limit=4097):\n",
        "    # Extract text with BeautifulSoup\n",
        "    text = bs(html, \"html.parser\").text\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text.strip())\n",
        "\n",
        "    # Approximate tokenization by splitting on spaces\n",
        "    tokens = text.split(' ')\n",
        "\n",
        "    # Limit the number of tokens and join back into a string\n",
        "    limited_text = ' '.join(tokens[:token_limit])\n",
        "    return limited_text\n",
        "\n",
        "def crawl_site(url):\n",
        "    loader = RecursiveUrlLoader(\n",
        "        url=url,\n",
        "        max_depth=2,\n",
        "        extractor=lambda x: extract_text_and_limit_tokens(x, 6000)\n",
        "    )\n",
        "    docs = loader.load()\n",
        "    return docs"
      ],
      "metadata": {
        "id": "ZEKFn9UDEmMS"
      },
      "id": "ZEKFn9UDEmMS",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [\n",
        "    Tool(\n",
        "        name = \"search\",\n",
        "        func=search.run,\n",
        "        description=\"Search the internet to find helpful websites.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Calculator\",\n",
        "        func=llm_math_chain.run,\n",
        "        description=\"Useful for when you need to answer questions about math\",\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"site_crawler\",\n",
        "        func=crawl_site,\n",
        "        description=\"Crawl a website up to depth 2.\"\n",
        "    ),\n",
        "]\n",
        "\n",
        "mrkl = initialize_agent(\n",
        "    agent=AgentType.OPENAI_FUNCTIONS,\n",
        "    tools=tools,\n",
        "    llm=turbo_llm,\n",
        "    verbose=True,\n",
        "    max_iterations=3,\n",
        "    early_stopping_method='generate',\n",
        ")"
      ],
      "metadata": {
        "id": "9h7l5_UrE6b-"
      },
      "id": "9h7l5_UrE6b-",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fixed_prompt = '''You are a helpful AI assistant.''' # can change this later\n",
        "mrkl.agent.prompt.messages[0].content = fixed_prompt\n",
        "mrkl.agent.prompt.messages[0].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4Umihm5HPA5l",
        "outputId": "aeaf627d-b691-4f31-ab6b-867b48fadfca"
      },
      "id": "4Umihm5HPA5l",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are a helpful AI assistant.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "location = \"Maple Street Guitars\"\n",
        "# result = mrkl.run(f\"Search the internet about {location} and crawl at most 5 websites to find the necessary information about the price and what people usually do or order in there.\")"
      ],
      "metadata": {
        "id": "q6VHBocdq6BK"
      },
      "id": "q6VHBocdq6BK",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(result)"
      ],
      "metadata": {
        "id": "Huj1p9wLQujn"
      },
      "id": "Huj1p9wLQujn",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# result"
      ],
      "metadata": {
        "id": "8JbZFVWx0pmG"
      },
      "id": "8JbZFVWx0pmG",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Searching maps"
      ],
      "metadata": {
        "id": "xZ1z5VckY637"
      },
      "id": "xZ1z5VckY637"
    },
    {
      "cell_type": "code",
      "source": [
        "from duckduckgo_search import DDGS\n",
        "\n",
        "ddgs = DDGS()\n",
        "\n",
        "# result = list(ddgs.maps('good restaurants around Atlanta, GA', max_results=50))"
      ],
      "metadata": {
        "id": "tGsKPLaSUuXq"
      },
      "id": "tGsKPLaSUuXq",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## List of recommended places"
      ],
      "metadata": {
        "id": "dNfBEvdBd8GS"
      },
      "id": "dNfBEvdBd8GS"
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "from openai import OpenAI\n",
        "\n",
        "def get_recommended_places(latitude=\"33.771030\", longitude= \"-84.391090\", radius=10): # defaults on north ave apartment\n",
        "    # List of keys to keep\n",
        "    keys_to_keep = ['title', 'address', 'latitude', 'longitude', 'phone', 'preference']\n",
        "    # List of user preferences\n",
        "    preferences_list = ['sports', 'art and culture', 'museum and history', 'food and dining', 'nature and outdoors', 'music', 'technology', 'shopping', 'movies and entertainment']\n",
        "    # List of recommended places based on the user preferences\n",
        "    recommended_places_list = []\n",
        "    for preference in preferences_list:\n",
        "        for original_dict in ddgs.maps(f\"places related to {preference}\", latitude=str(latitude), longitude= str(longitude), radius=radius, max_results=10):\n",
        "            # Add the 'preference' key and value directly to the original dictionary\n",
        "            original_dict['preference'] = preference\n",
        "            recommended_places_list.append({k: original_dict[k] for k in keys_to_keep if k in original_dict})\n",
        "    return recommended_places_list\n",
        "\n",
        "def generate_information(place_name, client):\n",
        "    prompt = f\"\"\"\n",
        "    Based on what you know, generate about this place: {place_name}.\n",
        "    Key information such as the environment and atmosphere of the place. If possible, estimate the range of the cost, and give some recommendations of what food people ordered or activities they did.\n",
        "    Label them appropriately, and go to the next line for each detail.\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-1106\",\n",
        "        max_tokens=200,\n",
        "        temperature=0,\n",
        "        messages=\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant.\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            },\n",
        "        ],\n",
        "\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "def thread_worker(place, client):\n",
        "    try:\n",
        "        place['generated_info'] = generate_information(place['title'], client)\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating information for {place['title']}: {e}\")\n",
        "\n",
        "# generate information using threading\n",
        "def process_places_concurrently(places, client):\n",
        "    threads = []\n",
        "    for place in places:\n",
        "        t = threading.Thread(target=thread_worker, args=(place, client))\n",
        "        threads.append(t)\n",
        "        t.start()\n",
        "\n",
        "    # Wait for all threads to complete\n",
        "    for t in threads:\n",
        "        t.join()\n",
        "\n",
        "    return places"
      ],
      "metadata": {
        "id": "q45MdjUwtGv5"
      },
      "id": "q45MdjUwtGv5",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "recommended_places_list = get_recommended_places(latitude=\"33.771030\", longitude= \"-84.391090\", radius=10)\n",
        "updated_recommended_places_list = process_places_concurrently(recommended_places_list, client)\n",
        "updated_recommended_places_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCA75hTJhGqP",
        "outputId": "e91cf114-7f4f-46e3-cbc2-feccd6b27ed1"
      },
      "id": "yCA75hTJhGqP",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'Rodney Cook Sr. Park',\n",
              "  'address': '609 Thurmond St NW, Atlanta, GA  30314, United States',\n",
              "  'latitude': 33.7617605739047,\n",
              "  'longitude': -84.4076693058014,\n",
              "  'phone': '+14048810900',\n",
              "  'preference': 'sports',\n",
              "  'generated_info': \"Rodney Cook Sr. Park is a beautiful urban park located in the historic Vine City neighborhood of Atlanta, Georgia. The park offers a peaceful and serene environment, with lush green spaces, walking trails, and a stunning water feature.\\n\\nAtmosphere: The atmosphere at Rodney Cook Sr. Park is relaxed and inviting, making it a perfect place for a leisurely stroll, a picnic, or simply enjoying the natural surroundings.\\n\\nCost: The park is free to enter and enjoy, making it an affordable option for a day out in Atlanta.\\n\\nFood: Visitors to Rodney Cook Sr. Park often bring picnic lunches to enjoy in the park's designated picnic areas. Popular food choices include sandwiches, salads, and fresh fruit.\\n\\nActivities: Common activities at the park include walking or jogging along the trails, birdwatching, and enjoying the water feature. The park also hosts community events and gatherings, providing opportunities for socializing and relaxation.\"},\n",
              " {'title': 'Atlanta Street Art Tours',\n",
              "  'address': '381 Edgewood Ave SE, Cafe + Velo, Atlanta, GA 30312-1839',\n",
              "  'latitude': 33.7542204,\n",
              "  'longitude': -84.3750217,\n",
              "  'phone': '+14043309320',\n",
              "  'preference': 'art and culture',\n",
              "  'generated_info': \"Atlanta Street Art Tours offers a vibrant and creative environment, showcasing the city's diverse and dynamic street art scene. The atmosphere is lively and engaging, providing a unique opportunity to explore the city's urban art landscape.\\n\\nCost: The range of cost for Atlanta Street Art Tours typically falls between $20 to $40 per person, depending on the duration and specific offerings of the tour.\\n\\nFood: After the tour, visitors often enjoy exploring the local food scene. Some popular recommendations include trying Southern comfort food such as fried chicken, biscuits, and collard greens, or sampling international cuisine at one of the city's many diverse restaurants.\\n\\nActivities: In addition to the street art tour, visitors to Atlanta often enjoy exploring the city's historic neighborhoods, visiting museums and galleries, and experiencing the vibrant nightlife and music scene.\"},\n",
              " {'title': 'SCAD Art Sales',\n",
              "  'address': '1600 Peachtree St NE, Atlanta, GA  30309, United States',\n",
              "  'latitude': 33.7968179,\n",
              "  'longitude': -84.3910238,\n",
              "  'phone': '',\n",
              "  'preference': 'art and culture',\n",
              "  'generated_info': 'SCAD Art Sales is an art gallery and retail space located in Savannah, Georgia, and Atlanta, Georgia. The environment is creative and vibrant, with a focus on showcasing the work of talented artists from the Savannah College of Art and Design (SCAD) community.\\n\\nAtmosphere:\\nThe atmosphere at SCAD Art Sales is sophisticated and inspiring, with a mix of contemporary and traditional art pieces. Visitors can expect a welcoming and inclusive environment that encourages exploration and appreciation of art.\\n\\nCost Range:\\nThe cost range for art pieces at SCAD Art Sales can vary widely, depending on the size, medium, and artist. Prices can range from affordable prints to higher-end original works of art.\\n\\nRecommendations:\\nFood: Visitors to SCAD Art Sales can explore the surrounding areas in Savannah or Atlanta to enjoy a variety of dining options, from trendy cafes to upscale restaurants. Local Southern cuisine is a popular choice, with dishes like shrimp and grits, fried green tomatoes, and pecan pie being favorites.\\n\\nActivities'},\n",
              " {'title': 'Tech Square',\n",
              "  'address': '79 5th St NW, Atlanta, GA  30308, United States',\n",
              "  'latitude': 33.777007,\n",
              "  'longitude': -84.389108,\n",
              "  'phone': '+14043306000',\n",
              "  'preference': 'technology',\n",
              "  'generated_info': 'Tech Square is a vibrant and bustling area located in the heart of Atlanta, Georgia. The environment is dynamic and innovative, with a mix of tech companies, startups, and academic institutions creating a lively and forward-thinking atmosphere.\\n\\nCost range: The cost range for dining and activities in Tech Square can vary, with options available for different budgets. Generally, expect mid-range to higher-end pricing for dining and entertainment.\\n\\nFood recommendations: Visitors to Tech Square often enjoy a diverse range of culinary experiences, from trendy cafes and food trucks to upscale restaurants. Popular food choices include artisanal coffee, gourmet sandwiches, fusion cuisine, and farm-to-table fare.\\n\\nActivity recommendations: Tech Square offers a range of activities, including visiting innovation centers, attending tech meetups, exploring interactive art installations, and enjoying live music and entertainment at local venues. Additionally, the area is known for its vibrant nightlife, with options for bars, lounges, and late-night dining.'},\n",
              " {'title': 'Piedmont Ear, Nose, Throat & Related Allergy',\n",
              "  'address': '1720 Peachtree St NW, Unit 200, Atlanta, GA 30309, United States',\n",
              "  'latitude': 33.8010341,\n",
              "  'longitude': -84.3933895,\n",
              "  'phone': '+14043515045',\n",
              "  'preference': 'shopping',\n",
              "  'generated_info': 'Piedmont Ear, Nose, Throat & Related Allergy is a medical facility specializing in ear, nose, throat, and allergy-related treatments. The environment is professional and welcoming, with a focus on patient comfort and care.\\n\\nCost range: The cost of services at Piedmont Ear, Nose, Throat & Related Allergy may vary depending on the specific treatment or procedure. It is recommended to contact the facility directly for pricing information.\\n\\nRecommendations:\\n- Food: As a medical facility, Piedmont Ear, Nose, Throat & Related Allergy does not offer food recommendations. However, patients and visitors may find nearby restaurants and cafes to enjoy a meal before or after their appointments.\\n- Activities: Patients may engage in reading, listening to music, or using their electronic devices while waiting for their appointments. The facility may also provide reading materials or educational resources related to ear, nose, throat, and allergy health.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating information concurrently"
      ],
      "metadata": {
        "id": "aIuJCxUWjVyZ"
      },
      "id": "aIuJCxUWjVyZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Websites result from place"
      ],
      "metadata": {
        "id": "TaOp0ZA_dkAV"
      },
      "id": "TaOp0ZA_dkAV"
    },
    {
      "cell_type": "code",
      "source": [
        "detailed_result_list = []\n",
        "for r in ddgs.text('R. Thomas Deluxe Grill', max_results=10):\n",
        "    detailed_result_list.append(r)\n",
        "detailed_result_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZWoi7yfeYc-",
        "outputId": "25b5effa-a066-4055-a6ef-5b82ea2facee"
      },
      "id": "tZWoi7yfeYc-",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'title': 'R. Thomas Deluxe Grill',\n",
              "  'href': 'https://www.rthomasdeluxegrill.net/',\n",
              "  'body': 'R. Thomas Deluxe Grill R. Thomas Deluxe Grill in Atlanta, GA. Food for everyone! We use fresh vegetables, fruit and meat across our menu. We have something yummy for every type of tummy. From burgers to vegan we aim to feed you. Write a Review, Win $500! Help guests by leaving a review of your favorite dishes.'},\n",
              " {'title': 'R. THOMAS DELUXE GRILL - 1009 Photos & 1167 Reviews - Yelp',\n",
              "  'href': 'https://www.yelp.com/biz/r-thomas-deluxe-grill-atlanta',\n",
              "  'body': 'R. Thomas Deluxe Grill 3.9 (1,167 reviews) Claimed $$ Vegetarian, Vegan, Breakfast & Brunch Open 7:00 AM - 11:00 PM See hours Verified by the business 2 months ago See all 1.0k photos Write a review Add photo Menu Popular dishes View full menu French Toast 43 Photos 168 Reviews Thai Express 14 Photos 56 Reviews Spicy Fish Tacos 19 Photos 49 Reviews'},\n",
              " {'title': 'R. Thomas Deluxe Grill - American Restaurant in Atlanta',\n",
              "  'href': 'https://rthomasdeluxegrill.business.site/',\n",
              "  'body': 'R. Thomas Deluxe Grill. American Restaurant in Atlanta. Open until 5:00 AM tomorrow. View Menu Call (404) 881-0246 Get directions Get Quote WhatsApp (404) 881-0246 Message (404) 881-0246 Contact Us Find Table Make Appointment Place Order. Testimonials.'},\n",
              " {'title': 'R. Thomas Deluxe Grill | Atlanta GA - Facebook',\n",
              "  'href': 'https://www.facebook.com/rthomasdeluxegrill/',\n",
              "  'body': 'R. Thomas Deluxe Grill, Atlanta, Georgia. 13,926 likes Â· 195 talking about this Â· 28,081 were here. est. 1985, Atlanta restaurant offering wholesome food...'},\n",
              " {'title': 'R. THOMAS DELUXE GRILL, Atlanta - Buckhead - Tripadvisor',\n",
              "  'href': 'https://www.tripadvisor.com/Restaurant_Review-g60898-d435430-Reviews-R_Thomas_Deluxe_Grill-Atlanta_Georgia.html',\n",
              "  'body': 'R. Thomas Deluxe Grill Claimed Review Save Share 290 reviews #112 of 1,921 Restaurants in Atlanta $$ - $$$ American Vegetarian Friendly Vegan Options 1812 Peachtree St NW, Atlanta, GA 30309-1858 +1 404-872-2942 Website Closed now : See all hours Improve this listing See all (150) Ratings and reviews 4.5 290 reviews #112 RATINGS Food Service Value'},\n",
              " {'title': 'R. THOMAS DELUXE GRILL - 999 Photos & 1154 Reviews - Yelp',\n",
              "  'href': 'https://www.yelp.com/biz/r-thomas-deluxe-grill-atlanta?start=200',\n",
              "  'body': 'R. Thomas Deluxe Grill 1154 reviews Claimed $$ Vegan, American (Traditional), Breakfast & Brunch Open 7:00 AM - 11:00 PM Hours updated over 3 months ago See hours See all 1011 photos Menu French Toast 43 Photos 166 Reviews Thai Express 14 Photos 56 Reviews Spicy Fish Tacos 18 Photos 48 Reviews Veggie Sausage 10 Photos 30 Reviews'},\n",
              " {'title': 'R Thomas Deluxe Grill Delivery Menu - DoorDash',\n",
              "  'href': 'https://www.doordash.com/store/r-thomas-deluxe-grill-atlanta-21361',\n",
              "  'body': 'Prices on this menu are set directly by the Merchant. Prices may differ between Delivery and Pickup. Get delivery or takeout from R Thomas Deluxe Grill at 1812 Peachtree Street Northwest in Atlanta. Order online and track your order live. No delivery fee on your first order!'},\n",
              " {'title': 'R. THOMAS DELUXE GRILL - 1000 Photos & 1156 Reviews - Yelp',\n",
              "  'href': 'https://www.yelp.com/biz/r-thomas-deluxe-grill-atlanta?start=60',\n",
              "  'body': \"If it's unique surroundings you're after though, R. Thomas Deluxe grill has that definitely going for itself! See all photos from Jamarcus T. for R. Thomas Deluxe Grill. Useful 19. Funny 17. Cool 16. Brittany S. Elite 2023. Charlotte, NC. 36. 149. 267. 2/18/2020. 1 photo. What a tasty, late night spot! Came here for late dinner around midnight ...\"},\n",
              " {'title': 'R. Thomas Deluxe Grill - Uber Eats',\n",
              "  'href': 'https://www.ubereats.com/store/r-thomas-deluxe-grill/iHnj4R99RR2zTeH3GFS6-w',\n",
              "  'body': 'R. Thomas Deluxe Grill. 4.6 (99 ratings) â€¢ Burgers. â€¢ More info. 1812 Peachtree St Nw, Atlanta, GA 30309-1858. Enter your address above to see fees, and delivery + pickup estimates.'},\n",
              " {'title': 'R. Thomas Deluxe Grill',\n",
              "  'href': 'https://www.rthomasdeluxegrill.net/menus/in-house-menu',\n",
              "  'body': 'Opens in a new window Opens an external site Opens an external site in a new window Opens an external site Opens an external site in a new window'}]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "detailed_result_list[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYfo9gBROAKg",
        "outputId": "bb5a7585-7c99-4423-f744-6b7ae6559b2b"
      },
      "id": "eYfo9gBROAKg",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'title': 'R. Thomas Deluxe Grill',\n",
              " 'href': 'https://www.rthomasdeluxegrill.net/',\n",
              " 'body': 'R. Thomas Deluxe Grill R. Thomas Deluxe Grill in Atlanta, GA. Food for everyone! We use fresh vegetables, fruit and meat across our menu. We have something yummy for every type of tummy. From burgers to vegan we aim to feed you. Write a Review, Win $500! Help guests by leaving a review of your favorite dishes.'}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crawl_site('https://www.rthomasdeluxegrill.net/') # not using because it does not crawl efficiently"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-RbQqZ2OalO",
        "outputId": "81dbd995-d45a-48e6-faa0-6947d01d5dc7"
      },
      "id": "m-RbQqZ2OalO",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Just a moment...Enable JavaScript and cookies to continue', metadata={'source': 'https://www.rthomasdeluxegrill.net/', 'title': 'Just a moment...', 'language': 'en-US'})]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4==4.11.2 Flask==3.0.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWomXTSwSJ_h",
        "outputId": "4119fa51-6c3b-4e66-b43b-9aee352f8be0"
      },
      "id": "OWomXTSwSJ_h",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4==4.11.2 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Collecting Flask==3.0.0\n",
            "  Using cached flask-3.0.0-py3-none-any.whl (99 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4==4.11.2) (2.5)\n",
            "Requirement already satisfied: Werkzeug>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from Flask==3.0.0) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask==3.0.0) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from Flask==3.0.0) (2.1.2)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from Flask==3.0.0) (8.1.7)\n",
            "Collecting blinker>=1.6.2 (from Flask==3.0.0)\n",
            "  Using cached blinker-1.7.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.1.2->Flask==3.0.0) (2.1.3)\n",
            "Installing collected packages: blinker, Flask\n",
            "  Attempting uninstall: blinker\n",
            "    Found existing installation: blinker 1.4\n",
            "\u001b[31mERROR: Cannot uninstall 'blinker'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install crochet==2.1.1\n",
        "!pip install Scrapy==2.11.0\n",
        "!pip install readability-lxml==0.8.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4d52oVGS2CB",
        "outputId": "dc5bb9af-e5e5-41e0-b675-2124d3f5809d"
      },
      "id": "O4d52oVGS2CB",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: crochet==2.1.1 in /usr/local/lib/python3.10/dist-packages (2.1.1)\n",
            "Requirement already satisfied: Twisted>=16.0 in /usr/local/lib/python3.10/dist-packages (from crochet==2.1.1) (22.10.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from crochet==2.1.1) (1.14.1)\n",
            "Requirement already satisfied: zope.interface>=4.4.2 in /usr/local/lib/python3.10/dist-packages (from Twisted>=16.0->crochet==2.1.1) (6.1)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.10/dist-packages (from Twisted>=16.0->crochet==2.1.1) (23.10.4)\n",
            "Requirement already satisfied: incremental>=21.3.0 in /usr/local/lib/python3.10/dist-packages (from Twisted>=16.0->crochet==2.1.1) (22.10.0)\n",
            "Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from Twisted>=16.0->crochet==2.1.1) (22.10.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.10/dist-packages (from Twisted>=16.0->crochet==2.1.1) (21.0.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from Twisted>=16.0->crochet==2.1.1) (23.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from Twisted>=16.0->crochet==2.1.1) (4.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from Automat>=0.8.0->Twisted>=16.0->crochet==2.1.1) (1.16.0)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.10/dist-packages (from hyperlink>=17.1.1->Twisted>=16.0->crochet==2.1.1) (3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface>=4.4.2->Twisted>=16.0->crochet==2.1.1) (67.7.2)\n",
            "Requirement already satisfied: Scrapy==2.11.0 in /usr/local/lib/python3.10/dist-packages (2.11.0)\n",
            "Requirement already satisfied: Twisted<23.8.0,>=18.9.0 in /usr/local/lib/python3.10/dist-packages (from Scrapy==2.11.0) (22.10.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from Scrapy==2.11.0) (41.0.5)\n",
            "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from Scrapy==2.11.0) (1.2.0)\n",
            "Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from Scrapy==2.11.0) (1.1.0)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from Scrapy==2.11.0) (1.8.1)\n",
            "Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from Scrapy==2.11.0) (23.3.0)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from Scrapy==2.11.0) (1.6.2)\n",
            "Requirement already satisfied: service-identity>=18.1.0 in /usr/local/lib/python3.10/dist-packages (from Scrapy==2.11.0) (23.1.0)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from Scrapy==2.11.0) (2.1.2)\n",
            "Requirement already satisfied: zope.interface>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from Scrapy==2.11.0) (6.1)\n",
            "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.10/dist-packages (from Scrapy==2.11.0) (0.3.0)\n",
            "Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from Scrapy==2.11.0) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from Scrapy==2.11.0) (67.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from Scrapy==2.11.0) (23.2)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.10/dist-packages (from Scrapy==2.11.0) (5.1.0)\n",
            "Requirement already satisfied: lxml>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from Scrapy==2.11.0) (4.9.3)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from Scrapy==2.11.0) (2.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->Scrapy==2.11.0) (1.16.0)\n",
            "Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.10/dist-packages (from itemloaders>=1.0.1->Scrapy==2.11.0) (1.0.1)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->Scrapy==2.11.0) (23.1.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->Scrapy==2.11.0) (0.5.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->Scrapy==2.11.0) (0.3.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->Scrapy==2.11.0) (23.10.4)\n",
            "Requirement already satisfied: incremental>=21.3.0 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->Scrapy==2.11.0) (22.10.0)\n",
            "Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->Scrapy==2.11.0) (22.10.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->Scrapy==2.11.0) (21.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->Scrapy==2.11.0) (4.5.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->Scrapy==2.11.0) (3.4)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract->Scrapy==2.11.0) (2.31.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract->Scrapy==2.11.0) (1.5.1)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->Scrapy==2.11.0) (3.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from Automat>=0.8.0->Twisted<23.8.0,>=18.9.0->Scrapy==2.11.0) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->Scrapy==2.11.0) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->Scrapy==2.11.0) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->Scrapy==2.11.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->Scrapy==2.11.0) (2023.7.22)\n",
            "Requirement already satisfied: readability-lxml==0.8.1 in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from readability-lxml==0.8.1) (5.2.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from readability-lxml==0.8.1) (4.9.3)\n",
            "Requirement already satisfied: cssselect in /usr/local/lib/python3.10/dist-packages (from readability-lxml==0.8.1) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import crochet\n",
        "crochet.setup()\n",
        "\n",
        "import bs4\n",
        "from duckduckgo_search import DDGS\n",
        "from openai import OpenAI\n",
        "import threading\n",
        "import queue\n",
        "from readability import Document\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerRunner\n",
        "from scrapy.settings import Settings"
      ],
      "metadata": {
        "id": "fyfWpF06RwZO"
      },
      "id": "fyfWpF06RwZO",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHUNK_SIZE = 13500\n",
        "CHUNK_OVERLAP = 100\n",
        "\n",
        "ddgs = DDGS()\n",
        "\n",
        "def extract_useful_information_from_single_chunk(client, url, title, text, ix, q=None):\n",
        "    '''\n",
        "    This function takes the url, title, and a chunk of text of a webpage, and it asks\n",
        "    openai to extract only the useful information from the text. It returns the result,\n",
        "    which is a string of text, and it also puts the result in a queue if a queue is passed in.\n",
        "    '''\n",
        "    # in this function, we will take the url, title, and some text extracted from the webpage\n",
        "    # by bs4, and we will ask openai to extract only the useful information from the text\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    You will be given information about a place. Your task is to extract and summarize the key information. If there is no information, simply return \"No Important Information Found\\n\".\n",
        "    Key information such as the environment and atmosphere of the place. If possible, estimate the range of the cost, and give some recommendations of what food people ordered or activities they did.\n",
        "    Try not to rewrite the text, but instead extract only the useful information from the text.\n",
        "\n",
        "    Here is a url: {url}\n",
        "    Here is its title: {title}\n",
        "    Here is some text extracted from the webpage:\n",
        "    {text}\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-1106\",\n",
        "        max_tokens=500,\n",
        "        temperature=0.2,\n",
        "        top_p=0.5,\n",
        "        frequency_penalty=0.3,\n",
        "        messages=\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant to help finding important information.\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            },\n",
        "        ],\n",
        "\n",
        "    )\n",
        "    if q:\n",
        "        q.put((ix, response.choices[0].message.content))\n",
        "\n",
        "    text = response.choices[0].message.content.strip()\n",
        "\n",
        "    # sometimes the first line is something like \"Useful information extracted from the text:\", so we remove that\n",
        "    lines = text.splitlines()\n",
        "    if \"useful information\" in lines[0].lower():\n",
        "        text = '\\n'.join(lines[1:])\n",
        "\n",
        "    return (ix, text)\n",
        "\n",
        "def extract_useful_information(client, url, title, text, max_chunks):\n",
        "    '''\n",
        "    This function takes the url, title, and text of a webpage.\n",
        "    It returns the most useful information from the text.\n",
        "\n",
        "    , and it calls\n",
        "    extract_useful_information_from_single_chunk to extract the useful information.\n",
        "\n",
        "    It does this by breaking the text into chunks, and then calling\n",
        "    extract_useful_information_from_single_chunk on each chunk (which is turn calls openai).\n",
        "    It then concatenates the results from all the chunks.\n",
        "\n",
        "    It uses threading to do this in parallel, because openai is slow.\n",
        "    '''\n",
        "    # Create the chunks with the specified size and overlap\n",
        "    chunks = [text[i:i+CHUNK_SIZE] for i in range(0, len(text), CHUNK_SIZE - CHUNK_OVERLAP)]\n",
        "    chunks = chunks[:max_chunks]\n",
        "\n",
        "    threads = []\n",
        "\n",
        "    q = queue.Queue()\n",
        "\n",
        "    for ix, chunk in enumerate(chunks):\n",
        "        t = threading.Thread(target=extract_useful_information_from_single_chunk, args=(client, url, title, chunk, ix, q))\n",
        "        threads.append(t)\n",
        "        t.start()\n",
        "\n",
        "    # Wait for all threads to complete\n",
        "    for t in threads:\n",
        "        t.join()\n",
        "\n",
        "    # Get all the results from the queue\n",
        "    results = []\n",
        "    while not q.empty():\n",
        "        results.append(q.get())\n",
        "\n",
        "    # Sort the results by the index\n",
        "    results.sort(key=lambda x: x[0])\n",
        "\n",
        "    # concatenate the text from the results\n",
        "    text = ''.join([x[1] for x in results])\n",
        "\n",
        "    return text\n",
        "\n",
        "def readability(input_text):\n",
        "    '''\n",
        "    This function will use the readability library to extract the useful information from the text.\n",
        "    Document is a class in the readability library. That library is (roughly) a python\n",
        "    port of readability.js, which is a javascript library that is used by firefox to\n",
        "    extract the useful information from a webpage. We will use the Document class to\n",
        "    extract the useful information from the text.\n",
        "    '''\n",
        "\n",
        "    doc = Document(input_text)\n",
        "\n",
        "    summary = doc.summary()\n",
        "\n",
        "    # the summary is html, so we will use bs4 to extract the text\n",
        "    soup = bs4.BeautifulSoup(summary, 'html.parser')\n",
        "    summary_text = soup.get_text()\n",
        "\n",
        "    return summary_text\n",
        "\n",
        "def remove_duplicate_empty_lines(text):\n",
        "    # Replace multiple spaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Replace multiple newlines with a single newline\n",
        "    text = re.sub(r'\\n+', '\\n', text)\n",
        "\n",
        "    # Replace multiple tabs with a single tab\n",
        "    text = re.sub(r'\\t+', '\\t', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "class MySpider(scrapy.Spider):\n",
        "    '''\n",
        "    This is the spider that will be used to crawl the webpages. We give this to the scrapy crawler.\n",
        "    '''\n",
        "    name = 'myspider'\n",
        "    start_urls = None\n",
        "    clean_with_llm = False\n",
        "    results = []\n",
        "\n",
        "    def __init__(self, start_urls, clean_with_llm, *args, **kwargs):\n",
        "        super(MySpider, self).__init__(*args, **kwargs)\n",
        "        self.start_urls = start_urls\n",
        "        self.clean_with_llm = clean_with_llm\n",
        "\n",
        "    def start_requests(self):\n",
        "        for url in self.start_urls:\n",
        "            yield scrapy.Request(url, callback=self.parse)\n",
        "\n",
        "    def parse(self, response):\n",
        "        body_html = response.body.decode('utf-8')\n",
        "\n",
        "        url = response.url\n",
        "\n",
        "        soup = bs4.BeautifulSoup(body_html, 'html.parser')\n",
        "        # Check if title tag exists\n",
        "        if soup.title:\n",
        "            title = soup.title.string\n",
        "        else:\n",
        "            title = \"No Title Found\"\n",
        "        text = soup.get_text()\n",
        "        text = remove_duplicate_empty_lines(text)\n",
        "\n",
        "        if self.clean_with_llm:\n",
        "            client = OpenAI(api_key=openai_api_key)\n",
        "            useful_text = extract_useful_information(client, url, title, text, 50)\n",
        "        else:\n",
        "            useful_text = readability(body_html)\n",
        "        useful_text = remove_duplicate_empty_lines(useful_text)\n",
        "\n",
        "        self.results.append({\n",
        "            'url': url,\n",
        "            'title': title,\n",
        "            # 'text': text,\n",
        "            'text': '',\n",
        "            'useful_text': useful_text\n",
        "        })\n",
        "\n",
        "\n",
        "\n",
        "@crochet.run_in_reactor\n",
        "def run_spider(url_list, clean_with_llm):\n",
        "    # Define custom settings\n",
        "    custom_settings = {\n",
        "        'LOG_ENABLED': False,  # disable logging.\n",
        "        'RANDOMIZE_DOWNLOAD_DELAY': True, # random .5 - 1.5 seconds\n",
        "    }\n",
        "\n",
        "    # Create a settings object\n",
        "    settings = Settings()\n",
        "    settings.setdict(custom_settings)\n",
        "\n",
        "    # Create a CrawlerRunner with the custom settings\n",
        "    crawler = CrawlerRunner(settings)\n",
        "    deferred = crawler.crawl(MySpider, start_urls=url_list, clean_with_llm=clean_with_llm)\n",
        "    return deferred\n",
        "\n",
        "def ddgsearch(query, numresults=10, clean_with_llm=False):\n",
        "    '''\n",
        "    This function performs a search on duckduckgo and returns the results.\n",
        "    It uses the scrapy library to download the pages and extract the useful information.\n",
        "    It extracts useful information from the pages using either the readability library\n",
        "    or openai, depending on the value of clean_with_llm.\n",
        "\n",
        "    query: the query to search for\n",
        "    numresults: the number of results to return\n",
        "    clean_with_llm: if True, use openai to clean the text. If False, use readability.\n",
        "    '''\n",
        "\n",
        "    # perform the search\n",
        "    results = list(ddgs.text(query, max_results=numresults))\n",
        "\n",
        "    # get the urls\n",
        "    urls = [result['href'] for result in results]\n",
        "    urls = urls[:numresults]\n",
        "\n",
        "    print(urls)\n",
        "    MySpider.results = []\n",
        "    eventual_result = run_spider(urls, clean_with_llm)\n",
        "\n",
        "    # Wait for the specified time or until the result is ready\n",
        "    try:\n",
        "        results = eventual_result.wait(timeout=20.0)\n",
        "    except crochet.TimeoutError:\n",
        "        raise Exception(\"The scraping operation timed out.\")\n",
        "\n",
        "    return MySpider.results\n",
        "\n",
        "def summarize_reviews(client, crawled_reviews, place_type):\n",
        "    all_reviews = ''\n",
        "    for crawled_review in crawled_reviews:\n",
        "        all_reviews += crawled_review['useful_text']\n",
        "\n",
        "    if (place_type == 'food and dining'):\n",
        "        prompt = f\"\"\"\n",
        "        \"From the following paragraph about a restaurant, please identify and summarize the key details regarding:\n",
        "        1) the estimated cost in the restaurant,\n",
        "        2) the most popular dishes or what people commonly order,\n",
        "        and 3) the environment and atmosphere of the restaurant.\n",
        "        {all_reviews}\"\n",
        "        \"\"\"\n",
        "    else:\n",
        "        prompt = f\"\"\"\n",
        "        \"From the following paragraph about a place, please identify and summarize the key details regarding:\n",
        "        1) the estimated cost of entry,\n",
        "        2) the most popular activitiess,\n",
        "        and 3) the environment and atmosphere of the place.\n",
        "        {all_reviews}\"\n",
        "        \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo-1106\",\n",
        "        max_tokens=500,\n",
        "        temperature=0,\n",
        "        messages=\n",
        "        [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant.\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            },\n",
        "        ],\n",
        "\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n"
      ],
      "metadata": {
        "id": "LfMjy_NLRwuY"
      },
      "id": "LfMjy_NLRwuY",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "place_name = \"R. Thomas Deluxe Grill\"\n",
        "place_type = \"food and dining\"\n",
        "results = ddgsearch(place_name, numresults=5, clean_with_llm=True)\n",
        "client = OpenAI(api_key=openai_api_key)\n",
        "summarize_reviews(client, results, place_type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "UPGstb6YLxJ_",
        "outputId": "b8209d17-7d42-42d5-d2e7-520f2be014fa"
      },
      "id": "UPGstb6YLxJ_",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1) The estimated cost in the restaurant is moderate ($$).\\n2) The most popular dishes or what people commonly order include the spicy fish tacos, \"The Champ\" fresh-made juice, big breakfast special with French toast and strawberries, French toast, Thai Express, and Mojo Jojo smoothie.\\n3) The environment and atmosphere of the restaurant is described as laid back with vibrant and whimsical decor, including caged parrots outside. It is known for its unique and fun atmosphere, with spot-on service and amazing food.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNuXQaDBiLvN",
        "outputId": "5fff507c-fd1a-4e6b-8cfb-bdd01fdc41e9"
      },
      "id": "YNuXQaDBiLvN",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'https://rthomasdeluxegrill.business.site/',\n",
              "  'title': 'R. Thomas Deluxe Grill - American Restaurant in Atlanta',\n",
              "  'text': '',\n",
              "  'useful_text': 'The R. Thomas Deluxe Grill in Atlanta is known for its unique and fun atmosphere, with spot-on service and amazing food. The restaurant is open until 5:00 AM on Thursdays, Fridays, and Saturdays. Customers have recommended the spicy fish tacos, \"The Champ\" fresh-made juice, and the big breakfast special with French toast and strawberries. The food is described as delicious and reasonably priced. No cost range was provided.'},\n",
              " {'url': 'https://www.facebook.com/rthomasdeluxegrill/',\n",
              "  'title': 'R. Thomas Deluxe Grill | Atlanta GA',\n",
              "  'text': '',\n",
              "  'useful_text': \"I'm sorry, but I cannot access external websites or URLs. Therefore, I am unable to extract information from the provided URL. If you have specific information about a place that you would like me to summarize, please provide the details directly.\"},\n",
              " {'url': 'https://www.yelp.com/biz/r-thomas-deluxe-grill-atlanta',\n",
              "  'title': 'R. THOMAS DELUXE GRILL - 1009 Photos & 1167 Reviews - 1812 Peachtree St NW, Atlanta, Georgia - Vegetarian - Restaurant Reviews - Phone Number - Yelp',\n",
              "  'text': '',\n",
              "  'useful_text': 'The R. Thomas Deluxe Grill in Atlanta, Georgia is a vegetarian and vegan-friendly restaurant that offers a diverse menu including breakfast items, burgers, wings, and fresh pressed juice. The atmosphere is described as laid back with vibrant and whimsical decor, including caged parrots outside. The restaurant is open from 7:00 AM to 11:00 PM, with extended hours until 5:00 AM on weekends. Customers recommend trying the French toast, Thai Express, Spicy Fish Tacos, and the Mojo Jojo smoothie. The cost is estimated to be moderate ($$). Overall, it is known for its eclectic atmosphere and diverse menu options.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "langchain",
      "language": "python",
      "name": "langchain"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "578e1e8dce4dc6c542f1ea2d66a2d9db6ef592936dcc314004bdae386f827d38"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}